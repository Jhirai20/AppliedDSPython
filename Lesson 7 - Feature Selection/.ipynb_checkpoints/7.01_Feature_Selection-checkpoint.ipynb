{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSFSfQrhpBIy"
   },
   "source": [
    "# **Feature Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFCTwQXWpFVD"
   },
   "source": [
    "## **Agenda**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGUyaIQPpNGI"
   },
   "source": [
    "In this lesson, we will cover the following concepts with the help of a business use case:\n",
    "* Feature Selection\n",
    "* Dimensionality Reduction:\n",
    "  * Dimensionality Reduction Techniques\n",
    "  * Pros and Cons of Dimensionality Reduction\n",
    "* Prominent Methods Used for Feature Selection:\n",
    "    * Factor Analysis\n",
    "    * Principal Component Analysis\n",
    "    * LDA\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmob5c_rLq7j"
   },
   "source": [
    "## **What Is Feature Selection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA2XJ5h1Lynm"
   },
   "source": [
    "Feature selection is a method that helps in the inclusion of the significant variables that<br> help form a model with good predictive power. \n",
    "\n",
    "Features or variables that are redundant or irrelevant can negatively impact the<br> performance of the model, thus it becomes necessary to remove them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Sr_qFfvbP2y"
   },
   "source": [
    "### **Benefits of Feature Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_0Mn4FKdRsm"
   },
   "source": [
    "* It reduces overfitting as the unwanted variables are removed, and the focus is on the significant variables.\n",
    "\n",
    "* It removes irrelevant information, which helps to improve the accuracy of the model’s predictions.\n",
    "\n",
    "* It reduces the computation time involved to get the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wrWVaY81T5Sc"
   },
   "source": [
    "## **Dimensionality Reduction**\n",
    "\n",
    "Dimensionality reduction is the method of transforming a collection of data having large dimensions into data of smaller dimensions while ensuring that identical information is conveyed concisely.\n",
    "\n",
    "### **Dimensionality Reduction Techniques**\n",
    "\n",
    "Some of the techniques used for dimensionality reduction are:\n",
    "\n",
    "1. Imputing missing values\n",
    "2. Dropping low-variance variables\n",
    "3. Decision trees (DT)\n",
    "4. Random forest (RF)\n",
    "5. Reducing highly correlated variables\n",
    "6. Backward feature elimination\n",
    "7. Factor analysis\n",
    "8. Principal component analysis (PCA)\n",
    "\n",
    "\n",
    "### **Pros of Dimensionality Reduction**\n",
    "\n",
    "- It helps to compress data, reducing the storage space needed.\n",
    "- It cuts down on computing time.\n",
    "- It also aids in the removal of redundant features.\n",
    "\n",
    "### **Cons of Dimensionality Reduction**\n",
    "\n",
    "- Some data may will be lost as a result.\n",
    "- PCA tends to find linear relationships between variables, which is not always ideal.\n",
    "- When mean and covariance are insufficient to describe datasets, PCA fails.\n",
    "- We use certain thumb rules when we do not know how many principal components to keep in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VxAtmkqzsY9-"
   },
   "source": [
    "## **Gist of Factor Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "omeHf4JJsazA"
   },
   "source": [
    "* Factor analysis is used to:\n",
    "  * Explain variance among the observed variables\n",
    "  * Condense the set of observed variables into the factors \n",
    "\n",
    "  ![FA](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/FA.JPG)\n",
    "\n",
    "* Factor explains the amount of variance in the observed variables.\n",
    "\n",
    "* In other words, factor analysis is a method that investigates linear relation of a number of variables of interest V1, V2,……., Vl, with a smaller number of unobservable factors F1, F2,..……, Fk.\n",
    "<br><br>\n",
    "  ![FA1](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/FA1.JPG)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PY30SMvIuOaU"
   },
   "source": [
    "### **Types of FA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzP9afLCuQ0r"
   },
   "source": [
    "![FA2](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/FA2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neVrajUnu-Gk"
   },
   "source": [
    "### **Work Process of FA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHgsVtDWyJIw"
   },
   "source": [
    "The objective of the factor analysis is the reduction of the number of observed variables and find the unobservable variables. \n",
    "\n",
    "It is a two-step process.\n",
    "\n",
    "![FA3](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/FA3.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WznRxFwz0xT0"
   },
   "source": [
    "### **Choosing Factors**\n",
    "\n",
    "Note: Let us get accustomed to the term eigenvalue before moving to the selecting the number of factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1ofi7fP0zC8"
   },
   "source": [
    "**Eigenvalues:**\n",
    "\n",
    "It represents the explained variance of each factor from the total variance and is also known as the characteristic roots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlYneF_q02Em"
   },
   "source": [
    "**Ways to Choose Factors:**\n",
    "\n",
    "* The eigenvalues are a good measure for identifying the significant factors. An eigenvalue greater than 1 is considered for the selection criteria of the feature.\n",
    "\n",
    "* Apart from observing values, the graphical approach is used that visually represents the factors' eigenvalues. This visualization is known as the scree plot. A Scree plot helps in determining the number of factors where the curve makes an elbow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jWLSxO2lce6l"
   },
   "source": [
    "### **Use Case: Feature Selection in Cancer Dataset Using FA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAdVv1cKce6n"
   },
   "source": [
    "**Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZJFc7mece6o"
   },
   "source": [
    "John Cancer Hospital (JCH) is a leading cancer hospital in the USA. It specializes in preventing breast cancer. \n",
    "\n",
    "Over the last few years, JCH has collected breast cancer data from patients who came for screening or treatment. \n",
    "\n",
    "However, this data has 32 attributes and is difficult to run and interpret the result. As an ML expert,\n",
    "\n",
    " you have to reduce the number of attributes so that the results are meaningful and accurate. \n",
    "\n",
    "Use FA for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Pf2xDB6ce6o"
   },
   "source": [
    "#### **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRcJ5Puvce6p"
   },
   "source": [
    "Features of the dataset are computed from a digitized image of a Fine-Needle Aspirate (FNA) of a breast mass. \n",
    "\n",
    "They describe the characteristics of the cell nuclei present in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTFnzjDRce6q"
   },
   "source": [
    "#### **Data Dictionary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gL5Y14LZce6r"
   },
   "source": [
    "**Dimensions:**\n",
    "* 32 variables\n",
    "* 569 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-_tp9Ypce6r"
   },
   "source": [
    "**Attribute Information:**\n",
    "\n",
    "1. ID number \n",
    "\n",
    "2. Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "3. Attributes with mean values: <br>\n",
    "10 real-valued features are computed for each cell nucleus:\n",
    "  * radius_mean (mean of distances from center to points on the perimeter)\n",
    "  * texture_mean (standard deviation of gray-scale values) \n",
    "  * perimeter_mean\n",
    "  * area_mean\n",
    "  * smoothness_mean (local variation in radius lengths)\n",
    "  * compactness_mean (perimeter$^2$ / area - 1.0)\n",
    "  * concavity_mean (severity of concave portions of the contour) \n",
    "  * concave points_mean (number of concave portions of the contour) \n",
    "  * symmetry_mean\n",
    "  * fractal dimension_mean (\"coastline approximation\" - 1)\n",
    "\n",
    "4. Attributes with standard error and worst/largest:\n",
    "  * radius_se\t\n",
    "  * texture_se\n",
    "  * perimeter_se\t\n",
    "  * area_se\t\n",
    "  * smoothness_se\t\n",
    "  * compactness_se\t\n",
    "  * concavity_se\t\n",
    "  * concave points_se\t\n",
    "  * symmetry_se\t\n",
    "  * fractal_dimension_se\t\n",
    "  * radius_worst\t\n",
    "  * texture_worst\t\n",
    "  * perimeter_worst\t\n",
    "  * area_worst\t\n",
    "  * smoothness_worst\t\n",
    "  * compactness_worst\t\n",
    "  * concavity_worst\t\n",
    "  * concave points_worst\t\n",
    "  * symmetry_worst\n",
    "  * fractal_dimension_worst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sNL6JVUEce6s"
   },
   "source": [
    "#### **Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDogpt2Sce6s"
   },
   "source": [
    "##### **Import Libraries**     \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-LinGPxaSsF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KVcSOGBabQH"
   },
   "source": [
    "In Python, Numpy is a package that includes multidimensional array objects as well as a number of derived objects.\n",
    "Matplotlib is an amazing visualization library in Python for 2D plots of arrays.\\n\n",
    "Pandas is used for data manipulation and analysis\\n\n",
    "So these are the core libraries that are used for the EDA process.\n",
    "\n",
    "These libraries are written with an import keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CP_NZx7gce6s"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9zbLtunce6v"
   },
   "source": [
    "##### **Import and Check the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5qNQpYxFabQJ"
   },
   "source": [
    "Before reading data from a csv file, you need to download the \"breast-cancer-data.csv\" dataset from the resource section and upload it into the Lab.\n",
    "We will use the Up arrow icon, which is shown on the left side under the View icon. Click on the Up arrow icon and upload the file from\n",
    "wherever it has downloaded into your system.\n",
    "\n",
    "After this, you will see the downloaded file will be visible on the left side of your lab along with all the .pynb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lVXiKc6ce6w"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('breast-cancer-data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hrscV9c7abQJ"
   },
   "source": [
    "pd.read_csv function is used to read the \"breast-cancer-data.csv\" file and df.head() will show the top 5 rows of the dataset.\n",
    "\n",
    "* dataframe or df is a variable that will store the data read by the csv file.\n",
    "* head will show the rows and () default take the 5 top rows as output.\n",
    "* one more example - df.head(3) will show the top 3 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UMyk59-abQJ"
   },
   "source": [
    "#### shape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tM3Y8scBce6z"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVUQa7FEabQK"
   },
   "source": [
    "df.shape will show the number of rows and columns in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQf4orHrabQK"
   },
   "source": [
    "#### info Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkH9HDtk2qjT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yLNdq2Sce61"
   },
   "outputs": [],
   "source": [
    "# Check the data , there should be no missing values \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-_Vad0wabQK"
   },
   "source": [
    "* The dataframe's information is printed using the info() function. \n",
    "* The number of columns, column labels, column data types, memory use, range index, and the number of cells in each column are all included in the data (non-null values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlRPCvp2ce62"
   },
   "outputs": [],
   "source": [
    "# defining the array as np.array\n",
    "feature_names = np.array(['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
    " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
    " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
    " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
    " 'smoothness error' 'compactness error' 'concavity error'\n",
    " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
    " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
    " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
    " 'worst concave points' 'worst symmetry' 'worst fractal dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IQVSwn2Xce64"
   },
   "outputs": [],
   "source": [
    "#### Convert diagnosis column to 1/0 and store in new column target\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2evEvFaUabQL"
   },
   "source": [
    "\n",
    "* The sklearn.preprocessing package contains a number of useful utility methods and transformer classes for converting raw feature vectors into a format that is suitable for downstream estimators.\n",
    "* LabelEncoder encodes labels with a value between 0 and n_classes-1 where n is the number of distinct labels.\n",
    "* These libraries are written with an import keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mi_rj_Yhce66"
   },
   "outputs": [],
   "source": [
    "# # Encode label diagnosis\n",
    "#M -> 1\n",
    "#B -> 0\n",
    "\n",
    "#Converting diagnosis to numerical variable in df\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0}).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCYMBTQ4abQL"
   },
   "source": [
    "In the above code, we are encoding the column diagnosis in which we are encoding M as 1 and B as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXqK7XiNabQM"
   },
   "source": [
    "## Factor Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fb0AXBpQabQM"
   },
   "source": [
    "* A linear statistical model is a factor analysis. \n",
    "* It is used to condense a group of observable variables into an unobserved variable termed factors and to explain the variance among the observed variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRvKiS6_ce7A"
   },
   "source": [
    "#### **Adequacy Test**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8dFOAxaabQM"
   },
   "source": [
    "Before you perform factor analysis, you need to evaluate the “factorability” of our dataset. Can we find the factors in the dataset? Checking factorability or sampling adequacy can be done in two ways:\n",
    "\n",
    "1- The Bartlett's Test\n",
    "\n",
    "2- Test of Kaiser-Meyer-Olkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9S0FafZhzej"
   },
   "outputs": [],
   "source": [
    "#Install factor analyzer\n",
    "#!pip install factor_analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7Ns2vbyjVtB"
   },
   "source": [
    "#### **Bartlett's Test**\n",
    "\n",
    "Bartlett’s test of sphericity checks whether or not the observed variables intercorrelate at all using the observed correlation matrix against the identity matrix. If the test is found to be statistically insignificant, you should not employ a factor analysis.\n",
    "\n",
    "Note: This test checks for the intercorrelation of observed variables by comparing the observed correlation matrix against the identity matrix.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kz3-9N9cabQN"
   },
   "outputs": [],
   "source": [
    "!pip install factor_analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59jRTLxLabQN"
   },
   "source": [
    "* In the above code, we are installing the factor analyzer.\n",
    "* pip is used to install the packages.\n",
    "* Factor analysis is an exploratory data analysis method used to search for influential underlying factors or latent variables from a set of observed variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9ZXXVQuabQN"
   },
   "source": [
    "Now, you are trying to perform factor analysis by using the factor analyzer module. Use the below code\n",
    "for calculating_bartlett_sphericity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaWYEa4Ifint"
   },
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value,p_value=calculate_bartlett_sphericity(df)\n",
    "chi_square_value, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46v4RI5PabQN"
   },
   "source": [
    "* In the above code, you are importing the factor analyzer and calculate_bartlett_sphericity.\n",
    "* In this Bartlett’s test, the p-value is 0. The test was statistically significant, indicating that the observed correlation matrix is not an identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bfr2qW3ZjRAc"
   },
   "source": [
    "**Inference:**\n",
    "\n",
    "The p-value is 0, and this indicates that the test is statistically significant and highlights that the correlation matrix is not an identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ArAG3cZEjlkn"
   },
   "source": [
    "#### **Kaiser-Meyer-Olkin Test**\n",
    "\n",
    "* The Kaiser-Meyer-Olkin (KMO) test determines if data is suitable for factor analysis.\n",
    "* It assesses the suitability of each observed variable as well as the entire model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nXMWtlgkce7A"
   },
   "outputs": [],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPTWaF7MmN07"
   },
   "outputs": [],
   "source": [
    "kmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YYgJghqCabQO"
   },
   "source": [
    "* In the above code, we are calculating the KMO. KMO estimates the proportion of variance among all the observed variables. \n",
    "* The overall KMO for the data is 0.25, which is excellent. This value indicates that you can proceed with your planned factor analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u40ACrK6oIHY"
   },
   "outputs": [],
   "source": [
    "df.head(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xE-3rJYCce7C"
   },
   "source": [
    "**Inference:**\n",
    "\n",
    "The KMO value is less than 0.5, and this indicates that we need to delete the insignificant variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5ARN-6mqKPF"
   },
   "source": [
    "**Finding Significant Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EAqKzACce7C"
   },
   "outputs": [],
   "source": [
    "corr = df.corr()\n",
    "corr.style.background_gradient(cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wntI4Lo_abQP"
   },
   "source": [
    "\n",
    "\n",
    "If your main goal is to visualize the correlation matrix rather than creating a plot per se, the convenient pandas styling options are a viable built-in solution, as shown in the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0u1-Jx_9sF9B"
   },
   "source": [
    "**Creating Dataset of Significant Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3dEgIDudrOxX"
   },
   "outputs": [],
   "source": [
    "df_corr = df[['radius_mean','perimeter_mean', 'area_mean','radius_worst','perimeter_worst','area_worst','concavity_mean','concave points_mean', 'concavity_worst','concave points_worst','diagnosis']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UCx9ELIsUK-"
   },
   "source": [
    "**Running KMO Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8cJuObwfr20a"
   },
   "outputs": [],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(df_corr)\n",
    "kmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2eAjgSynabQQ"
   },
   "source": [
    "* In the above code, we are calculating the KMO. KMO estimates the proportion of variance among all the observed variables. \n",
    "* The overall KMO for the data is 0.82, which is excellent. This value indicates that you can proceed with your planned factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1za8nwW3jbB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "meiYWjRdce7R"
   },
   "source": [
    "**Inference:**\n",
    "\n",
    "The value of the KMO model is greater than 0.5. Therefore, the dataset is good enough for factor analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8M3sIztt30d"
   },
   "source": [
    "##### **Factor Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx9sn4PBuA6W"
   },
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "fa = FactorAnalyzer(rotation='varimax')\n",
    "fa.fit(df_corr,11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0NCc4BlabQS"
   },
   "source": [
    "In the above code, we create a factor analysis object and perform factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUU8fESxwZFe"
   },
   "outputs": [],
   "source": [
    "ev, v = fa.get_eigenvalues()\n",
    "ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "symxLmsnabQS"
   },
   "source": [
    "* Here, you can see only for 6-factors eigenvalues are greater than one. It means we only need to choose 6 factors (or unobserved variables).\n",
    "* In the above code, we are checking the Eigenvalues.\n",
    "* It measures how much of the variance of the variables a factor explains.\n",
    "* An eigenvalue of more than one means that the factor explains more variance than a unique variable.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCmO6m1_1ERg"
   },
   "outputs": [],
   "source": [
    "plt.scatter(range(1,df_corr.shape[1]+1),ev)\n",
    "plt.plot(range(1,df_corr.shape[1]+1),ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D2k3xYUSabQS"
   },
   "source": [
    "In the above code, we are creating screen plot using matplotlib module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ2zQFbhxcL7"
   },
   "source": [
    "**Inference:**\n",
    "\n",
    "* Referring eigenvalues: There are only two values above 1. Therefore, we will choose only 2 factors (or unobserved variables).\n",
    "\n",
    "* Referring Scree plot: There are only two values after the elbow. Therefore, we will choose only 2 factors (or unobserved variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQ_ocrSO_s-o"
   },
   "outputs": [],
   "source": [
    "fa1 = FactorAnalyzer(rotation=\"varimax\", n_factors=2)\n",
    "fa1.fit(df, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0fa5hpF___bJ"
   },
   "outputs": [],
   "source": [
    "fa1.loadings_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odbmkgjZ0AUU"
   },
   "source": [
    "##### **Confirmatory Factor Analysis**\n",
    "\n",
    "* It is used to determine the factors and factor loadings of measured variables.\n",
    "* The general assumption of CFA is that every factor has an association with a specific subset of the measured variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WstnD140D54"
   },
   "outputs": [],
   "source": [
    "from factor_analyzer import (ConfirmatoryFactorAnalyzer, ModelSpecificationParser)    \n",
    "\n",
    "model_dict = {\"F1\": ['radius_mean','perimeter_mean', 'area_mean','radius_worst','perimeter_worst','diagnosis'], \"F2\": ['area_worst','concavity_mean','concave points_mean', 'concavity_worst','concave points_worst']}\n",
    "\n",
    "model_spec = ModelSpecificationParser.parse_model_specification_from_dict(df_corr,model_dict)\n",
    "\n",
    "\n",
    "cfa1 = ConfirmatoryFactorAnalyzer(model_spec, disp=False) \n",
    "cfa1.fit(df_corr.values) \n",
    "\n",
    "cfa1.loadings_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ufmAJKVR41_j"
   },
   "outputs": [],
   "source": [
    "cfa1.factor_varcovs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpEes4kw44aP"
   },
   "outputs": [],
   "source": [
    "cfa1.transform(df_corr.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJGLqxzXzyDZ"
   },
   "source": [
    "Now, let us take a look at PCA and LDA-specific benefits. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjw7joWC066i"
   },
   "source": [
    "## **PCA and LDA**                  #  NO Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPBRxNIu84TF"
   },
   "source": [
    "![PCA vs. LDA](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/PCA%20vs.%20LDA.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zusBMWTg_eDL"
   },
   "source": [
    "## **Gist of PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ikf5R3-Q_ipc"
   },
   "source": [
    "![PCA scenario](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/PCA%20scenario.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YXTpZxkSEzb7"
   },
   "source": [
    "### **What Is Principal Component?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4RtQQaVE53k"
   },
   "source": [
    "A principal component is a normalized linear combination of the original predictors in a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk8RObmqI8FR"
   },
   "source": [
    "#### **First Principal Component (Z$^1$)** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QwzizxZHFrL"
   },
   "source": [
    "* The aim of PCA is to find components that account for **maximum variance in the data** that includes the error and within-variable variance. \n",
    "\n",
    "* It finds the direction of the highest variability in the data. Greater the variability captured in the first component, the greater the information captured by the component.\n",
    "\n",
    "* The first principal component develops a line that is closest to the data points.<br>\n",
    "\n",
    "  In other words, it minimizes the sum of squared distance between a data point and the line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TFxO1KbrH5_m"
   },
   "source": [
    "**Equation for first principal component:**<br>\n",
    "\n",
    "**Z¹ = Φ¹¹X¹ + Φ²¹X² + Φ³¹X³ + .... +Φp¹Xp**\n",
    "\n",
    "  * Z¹ = First principal component\n",
    "  * Φp¹ = Loading vector \n",
    "  * X¹ to Xp = Normalized predictors that have mean equals to **0** and standard deviation equals to **1**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LXbw0cCmJAax"
   },
   "source": [
    "#### **Second Principal Component (Z$^2$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5bGa1ZrPHY_2"
   },
   "source": [
    "The aim of the linear combination of original predictors is to capture the remaining variance in the dataset where Z² is uncorrelated to Z¹. \n",
    "\n",
    "In other words, the correlation between the first and second components should be zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LOmAMK5RIB2U"
   },
   "source": [
    "**Equation for first principal component:**<br>\n",
    "\n",
    "**Z² = Φ¹²X¹ + Φ²²X² + Φ³²X³ + .... + Φp2Xp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LahRW2KhTrOP"
   },
   "source": [
    "#### **Graphical Representation of Uncorrelated Principal Components:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ABeq8rc1Ungw"
   },
   "source": [
    "![PC1 and PC2](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/PC1%20and%20PC2.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8sxp-1znS_TS"
   },
   "source": [
    "When we talk of all the succeeding principal components, they will follow a similar concept, i.e., capturing the remaining variation without being correlated to the previous component.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br4M3OWrkkag"
   },
   "source": [
    "### **How Does PCA work?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEvMh2caktiR"
   },
   "source": [
    "PCA is built on the concept of Eigen vectors and Eigen values. So, let us first understand the Eigen vector and value in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gu6tYTq4YadX"
   },
   "source": [
    "#### **Eigenvectors and Eigenvalues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-KaHCcrvYitA"
   },
   "source": [
    "##### **Eigenvectors**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psOwtlGZYo8K"
   },
   "source": [
    "When the vectors are plotted on a two-dimensional plane they showcase both the magnitude and direction. \n",
    "\n",
    "Usually, the direction of a vector is along its span but there are special vectors that on linear transformation get squished or stretched instead of falling off their span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21wEA9UeZ0Xp"
   },
   "source": [
    "##### **Eigenvalues**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xhvytO0YhLP"
   },
   "source": [
    "These are the constant values that increase or decrease the Eigenvectors along their span when transformed linearly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cbeb4FVwk2Qs"
   },
   "source": [
    "#### **PCA Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ga3rmKkTWZmj"
   },
   "source": [
    "###### **Finding PC1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFY9jBjZWR0G"
   },
   "source": [
    "![Find PC1](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/Find%20PC1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8q7IqaIyWi0c"
   },
   "source": [
    "###### **Result of Eigen Decomposition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FuWmcIV5W77A"
   },
   "source": [
    "![Eigenvalue Decopm](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/Eigenvalue%20decomp.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s2QJhWCzX9Dq"
   },
   "source": [
    "\n",
    "![eigenvalue](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/eigenvalue.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJAKTVXr0AFg"
   },
   "source": [
    "### **Use Case: Feature Selection in Cancer Dataset Using PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnGoEA-xwSpC"
   },
   "source": [
    "**Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2q2VcowxeaG"
   },
   "source": [
    "John Cancer Hospital (JCH) is a leading cancer hospital in the USA. It specializes in preventing breast cancer. \n",
    "\n",
    "Over the last few years, JCH has collected breast cancer data from patients who came for screening or treatment. \n",
    "\n",
    "However, this data has 32 attributes and is difficult to run and interpret the result. As an ML expert,\n",
    "\n",
    " you have to reduce the number of attributes so that the results are meaningful and accurate. \n",
    "\n",
    "Use PCA for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pbzSqLppxfKB"
   },
   "source": [
    "#### **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k04SjF2lxT9V"
   },
   "source": [
    "Features of the dataset are computed from a digitized image of a Fine-Needle Aspirate (FNA) of a breast mass. \n",
    "\n",
    "They describe the characteristics of the cell nuclei present in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxWJJg5s7CQw"
   },
   "source": [
    "##### **Links to the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZUfrIMWn5LPW"
   },
   "source": [
    "This database is available at:\n",
    "* The UCI Machine Learning Repository: <br>\n",
    "https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+%28Diagnostic%29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YmU1CSR5Z6A"
   },
   "source": [
    "#### **Data Dictionary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95DEP_OsK7-3"
   },
   "source": [
    "**Dimensions:**\n",
    "* 32 variables\n",
    "* 569 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ajcz39Cm5Skp"
   },
   "source": [
    "**Attribute Information:**\n",
    "\n",
    "1. ID number \n",
    "\n",
    "2. Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "3. Attributes with mean values: <br>\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "  * radius_mean (mean of distances from center to points on the perimeter)\n",
    "  * texture_mean (standard deviation of gray-scale values) \n",
    "  * perimeter_mean\n",
    "  * area_mean\n",
    "  * smoothness_mean (local variation in radius lengths)\n",
    "  * compactness_mean (perimeter$^2$ / area - 1.0)\n",
    "  * concavity_mean (severity of concave portions of the contour) \n",
    "  * concave points_mean (number of concave portions of the contour) \n",
    "  * symmetry_mean\n",
    "  * fractal dimension_mean (\"coastline approximation\" - 1)\n",
    "\n",
    "4. Attributes with standard error and worst/largest:\n",
    "  * radius_se\t\n",
    "  * texture_se\n",
    "  * perimeter_se\t\n",
    "  * area_se\t\n",
    "  * smoothness_se\t\n",
    "  * compactness_se\t\n",
    "  * concavity_se\t\n",
    "  * concave points_se\t\n",
    "  * symmetry_se\t\n",
    "  * fractal_dimension_se\t\n",
    "  * radius_worst\t\n",
    "  * texture_worst\t\n",
    "  * perimeter_worst\t\n",
    "  * area_worst\t\n",
    "  * smoothness_worst\t\n",
    "  * compactness_worst\t\n",
    "  * concavity_worst\t\n",
    "  * concave points_worst\t\n",
    "  * symmetry_worst\n",
    "  * fractal_dimension_worst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeFqVj8-7Svw"
   },
   "source": [
    "#### **Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PUkFpe7P0qF2"
   },
   "source": [
    "##### **Import Libraries**\n",
    "\n",
    "> Indented block        # CE Review\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7a2l8IJwdz9"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKa3Gw_XwkzY"
   },
   "source": [
    "##### **Import and Check the Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNJZojwVabQb"
   },
   "source": [
    "Before reading data from a csv file, you need to download the \"breast-cancer-data.csv\" dataset from the resource section and upload it into the lab.\n",
    "We will use the Up arrow icon which is shown on the left side under the View icon. Click on the Up arrow icon and upload the filewherever \n",
    "it has been downloaded into your system.\n",
    "\n",
    "After this, you will see the downloaded file will be visible on the left side of your lab along with all the .pynb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cooXVKkQwn_U"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('breast-cancer-data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbFtqJCJabQb"
   },
   "source": [
    "pd.read_csv function is used to read the \"breast-cancer-data.csv\" file and df.head() will show the top 5 rows of the dataset.\n",
    "\n",
    "* dataframe or df  is a variable that will store the data read by the csv file.\n",
    "* head will show the rows and () default take the top 5 rows as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3fntXDUXhWFT"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5kt2wp7NwtE-"
   },
   "outputs": [],
   "source": [
    "# Check the data , there should be no missing values \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4kb8U2babQc"
   },
   "source": [
    "* The dataframe's information is printed using the info() function. \n",
    "* The number of columns, column labels, column data types, memory use, range index, and the number of cells in   each column are all included in the data (non-null values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-LBaMGjw0up"
   },
   "outputs": [],
   "source": [
    "feature_names = np.array(['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
    " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
    " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
    " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
    " 'smoothness error' 'compactness error' 'concavity error'\n",
    " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
    " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
    " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
    " 'worst concave points' 'worst symmetry' 'worst fractal dimension'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tSrF_Rhzw8Xf"
   },
   "outputs": [],
   "source": [
    "#### Convert diagnosis column to 1/0 and store in new column target\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r2adM2bxJEJ"
   },
   "outputs": [],
   "source": [
    "# # Encode label diagnosis\n",
    "#M -> 1\n",
    "#B -> 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HwQ_qcemxJrA"
   },
   "outputs": [],
   "source": [
    "target_data=df[\"diagnosis\"]\n",
    "encoder = LabelEncoder()\n",
    "target_data = encoder.fit_transform(target_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfiZhK1LabQd"
   },
   "source": [
    "In the above code, in output we are getting all the rows, but only with the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-VlWpZAsxQ7G"
   },
   "outputs": [],
   "source": [
    "df.drop([\"diagnosis\"],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFLHmT02UFKZ"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFTG_BqqabQd"
   },
   "source": [
    "In the above code, you store the encoded column in a dataframe and drop the diagnosis column for simpilcity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64rGwpGexVeH"
   },
   "source": [
    "##### **Principal Component Analysis**\n",
    "\n",
    "Let's use PCA to find the first two principal components and visualize the data in this new, two-dimensional space with a single scatter-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYLwTcyo-tCd"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW7L4FHMabQd"
   },
   "source": [
    "In the above code, you will scale data so that each feature has a single unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwa9M6bQ-zNw"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CMovjOl5-1Pm"
   },
   "outputs": [],
   "source": [
    "scaled_data = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FS_Sm9aabQe"
   },
   "source": [
    "In the above code, you will initially create an object of the StandardScaler() function. Further, you will use fit() along with the object assigned to df and standardize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JosA28hnAjq3"
   },
   "source": [
    "###### **Two Principal Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qtnGsJrV-3yB"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzALdqr3abQe"
   },
   "source": [
    "In the above code, you can transform this data into its first 2 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLpKqZaj-7BW"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwOnx9W3abQf"
   },
   "source": [
    "Now, you will pass the number of components (n_components=2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TuoIPHU2--Gw"
   },
   "outputs": [],
   "source": [
    "pca.fit(scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GokrRt0d_AzT"
   },
   "outputs": [],
   "source": [
    "x_pca = pca.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KAewA2u7abQf"
   },
   "source": [
    "Finally, call fit function to aggregate the data. Here, several components represent the lower dimension in which you will project your higher dimension data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXYnVUEF_Cby"
   },
   "outputs": [],
   "source": [
    "#shape of data \n",
    "scaled_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aCsvA7l4_ThM"
   },
   "outputs": [],
   "source": [
    "x_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lfd1Bha3_b_C"
   },
   "source": [
    "**Plot**   # NO Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rVgf1GJz_Nxa"
   },
   "outputs": [],
   "source": [
    "# Reduced 30 dimensions to just 2! Let's plot these two dimensions out!\n",
    "# Draw inference from the plot?\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.scatter(x_pca[:,0],x_pca[:,1],c=target_data,cmap='viridis')\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQZSkCFBwcMP"
   },
   "source": [
    "**Interpreting the components:**\n",
    "\n",
    "Unfortunately, with this great power of dimensionality reduction, comes the cost of being able to easily understand what these components represent.\n",
    "\n",
    "The components correspond to combinations of the original features. The components themselves are stored as an attribute of the fitted PCA object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQe6DR4j_8Kc"
   },
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DltT566wfcM"
   },
   "source": [
    "**Explained Variance:**\n",
    "\n",
    "The explained variance tells you how much information (variance) can be attributed to each of the principal components. This is important as you can convert n-dimensional space to 2-dimensional space; you lose some of the variance (information)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-jPpt-HARxY"
   },
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cz7HtJYAXz3"
   },
   "source": [
    "###### **Three Principal Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QkMkwFvA3TQ"
   },
   "outputs": [],
   "source": [
    "pca_3 = PCA(n_components=3)\n",
    "pca_3.fit(scaled_data)\n",
    "x_pca_3 = pca_3.transform(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40vyleINBXLC"
   },
   "source": [
    "In this Numpy matrix array, each row represents a principal component, and each column relates back to the original features. You can visualize this relationship with a heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_st4_poBUMM"
   },
   "outputs": [],
   "source": [
    "x_pca_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TH4BFXAvA07e"
   },
   "source": [
    "**What is the total variance attributed by three Components?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z76NmvBxB0_G"
   },
   "outputs": [],
   "source": [
    "pca_3.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4oQTErVYZ-0x"
   },
   "source": [
    "## **Gist of LDA** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9Zmemx6aEIr"
   },
   "source": [
    "LDA identifies the linear combination of the observed variables that maximize\n",
    "the class<br> separation on the availability of the prior information of the classes.\n",
    "\n",
    "**For example:**<br>\n",
    "A variable in the training dataset that specifies the class of each observation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AsO_qi71wBn2"
   },
   "source": [
    "### **LDA Process**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGwzNHcsuAW1"
   },
   "source": [
    "* Assume a set of D - dimensional samples {X(1, X(2, …, X(N}, N1 of which belong to class ω1 and N2 to class ω2\n",
    "* Obtain a scalar **y** by projecting the samples **x** onto a line: **Y = W$^T$ X**\n",
    "* Of all the possible lines, select the one that maximizes the separability of the scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MzukDvb9wGYY"
   },
   "source": [
    "### **Line of Maximum Separability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tyAW-PGcv_nQ"
   },
   "source": [
    "The maximum separable line finds out the feature subspace such that class separability is also optimized.\n",
    "\n",
    "![Max S line](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/Max%20S%20line.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LSRhRKcD4UO"
   },
   "source": [
    "### **Finding Maximum Separable Line**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd9w8O84D7ed"
   },
   "source": [
    "#### **1. Measure of Separation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh_m2tYHEugx"
   },
   "source": [
    "![MSL1](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/MSL1.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIQDMeQ6GBP7"
   },
   "source": [
    "#### **2. Linear Discriminant**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emx55GzZFqls"
   },
   "source": [
    "![MSL2](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/MSL2.JPG)\n",
    "\n",
    "Note: Variables from same class are projected very close to each other and at the same time, the projected means are as farther apart as possible\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ha3o4Ue-Gfjs"
   },
   "source": [
    "#### **3. Optimum Projection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IA4DGgtdGifX"
   },
   "source": [
    "![MSL3](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/MSL3.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b4VYUorfIG7S"
   },
   "source": [
    "#### **4. Obtain the Maxima**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyNQm_dyINie"
   },
   "source": [
    "![MSL4](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Applied_Machine_Learning/Images/0.6_Feature_Selection/Trainer_PPT_and_IPYNB/MSL4.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JCpPKcheJp_S"
   },
   "source": [
    "Let us perform LDA using the same dataset that we used for PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIjDR5IxKCjV"
   },
   "source": [
    "### **Use Case: Feature Selection in Cancer Dataset Using LDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kFEot95iKOTP"
   },
   "source": [
    "Note: We are leveraging the problem statement, dataset, and data dictionary from the previous use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2GwCTiYJ4Hy-"
   },
   "source": [
    "#### **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "poRkAj0T4Laa"
   },
   "source": [
    "John Cancer Hospital (JCH) is a leading cancer hospital in the USA. It specializes in preventing breast cancer. \n",
    "\n",
    "Over the last few years, JCH has collected breast cancer data from patients who came for screening or treatment. \n",
    "\n",
    "However, this data has 32 attributes and is difficult to run and interpret the result. As an ML expert,\n",
    "\n",
    " you have to reduce the number of attributes so that the results are meaningful and accurate. \n",
    "\n",
    " Use LDA for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kWkqbieY4cmj"
   },
   "source": [
    "#### **Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N34j4EX44gWz"
   },
   "source": [
    "Features of the dataset are computed from a digitized image of a Fine-Needle Aspirate (FNA) of a breast mass. \n",
    "\n",
    "They describe the characteristics of the cell nuclei present in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iuc3JR44j_j"
   },
   "source": [
    "#### **Data Dictionary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9CLyCeyZ4keo"
   },
   "source": [
    "**Dimensions:**\n",
    "* 32 variables\n",
    "* 569 observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z2MFYINo4m95"
   },
   "source": [
    "**Attribute Information:**\n",
    "\n",
    "1. ID number \n",
    "\n",
    "2. Diagnosis (M = malignant, B = benign)\n",
    "\n",
    "3. Attributes with mean values: <br>\n",
    "Ten real-valued features are computed for each cell nucleus:\n",
    "  * radius_mean (mean of distances from center to points on the perimeter)\n",
    "  * texture_mean (standard deviation of gray-scale values) \n",
    "  * perimeter_mean\n",
    "  * area_mean\n",
    "  * smoothness_mean (local variation in radius lengths)\n",
    "  * compactness_mean (perimeter$^2$ / area - 1.0)\n",
    "  * concavity_mean (severity of concave portions of the contour) \n",
    "  * concave points_mean (number of concave portions of the contour) \n",
    "  * symmetry_mean\n",
    "  * fractal dimension_mean (\"coastline approximation\" - 1)\n",
    "\n",
    "4. Attributes with standard error and worst/largest:\n",
    "  * radius_se\t\n",
    "  * texture_se\n",
    "  * perimeter_se\t\n",
    "  * area_se\t\n",
    "  * smoothness_se\t\n",
    "  * compactness_se\t\n",
    "  * concavity_se\t\n",
    "  * concave points_se\t\n",
    "  * symmetry_se\t\n",
    "  * fractal_dimension_se\t\n",
    "  * radius_worst\t\n",
    "  * texture_worst\t\n",
    "  * perimeter_worst\t\n",
    "  * area_worst\t\n",
    "  * smoothness_worst\t\n",
    "  * compactness_worst\t\n",
    "  * concavity_worst\t\n",
    "  * concave points_worst\t\n",
    "  * symmetry_worst\n",
    "  * fractal_dimension_worst\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sG04PZQG4uXR"
   },
   "source": [
    "#### **Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bx6dwqAAGJf2"
   },
   "source": [
    "##### **Import and Check the Data**  # CE Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPe9kQa3fNKw"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold, cross_val_score, GridSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqnSgJ94abQq"
   },
   "source": [
    "In the above code, you are importing different files: pandas, numpy, pyplot, LinearDiscriminantAnalysis, KFold, cross_val_score, GridSearchCV, train_test_split,accuracy_score, classification_report, confusion_matrix and seaborn. To get to know them better, you can start the notebook from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q79TV8lAfNqh"
   },
   "outputs": [],
   "source": [
    "\n",
    "# load data\n",
    "\n",
    "df = pd.read_csv('breast-cancer-data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odsPvUeoabQr"
   },
   "source": [
    "Before reading data from a csv file, you need to download the \"breast-cancer-data.csv\" dataset from the resource section and upload it into the lab.\n",
    "We will use the Up arrow icon, which is shown on the left side under the View icon. Click on the Up arrow icon and upload the filewhereever it has been downloaded into your system.\n",
    "\n",
    "After this, you will see the downloaded file will be visible on the left side of your lab with all the .pynb files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrUnze6qfPrB"
   },
   "outputs": [],
   "source": [
    "\n",
    "df = df.head(559)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKubCVWffRwL"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k2eN0nxFabQs"
   },
   "source": [
    "df.shape will show the number of rows and columns in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SG0s7Q2afWi2"
   },
   "outputs": [],
   "source": [
    "# map categorical variable 'diagnosis' into numeric\n",
    "\n",
    "df.diagnosis = df.diagnosis.map({'M': 1, 'B': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0C0bb-2abQs"
   },
   "source": [
    "In the above code, you are encoding the column diagnosis in which we are encoding M as 1 and B as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdSR7qLEfYry"
   },
   "outputs": [],
   "source": [
    "# head will show the rows and () default take 5 top rows as output.\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B30VKmvafbCA"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fa3aGeFwfeP6"
   },
   "outputs": [],
   "source": [
    "# Drop redundant column 'id'\n",
    "\n",
    "df.drop('id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVrGs5kdfea9"
   },
   "outputs": [],
   "source": [
    "# Check for NA values\n",
    "\n",
    "df.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnFJGb9MfcO7"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df.iloc[:,:-1], df['diagnosis'], train_size=0.8, test_size=0.2, random_state=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Poimqol4abQu"
   },
   "source": [
    "In the above code, you split the data into training and validation sets (since we already have a separate test set).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uM1mKJQfcdO"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "norm = Normalizer()\n",
    "norm.fit(X_train)\n",
    "X_train_norm = norm.transform(X_train)\n",
    "X_val_norm = norm.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WjDrhJnSabQz"
   },
   "source": [
    "In above code, you are Normalizing the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUDnp3AB8CJX"
   },
   "source": [
    "##### **LDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fg1HvU17r-JV"
   },
   "source": [
    "Let's throw in linear classifiers, i.e., Linear Discriminant Analysis for good measure and because our dataset is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUXTLA4Yfcvu"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train_norm, y_train)\n",
    "lda_predicted = lda.predict(X_val_norm)\n",
    "\n",
    "print('LDA Accuracy is: {}'.format(accuracy_score(y_val,lda_predicted)))\n",
    "\n",
    "print('LDA Classification Report')\n",
    "print(classification_report(y_val, lda_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jAoo9GaHabQ0"
   },
   "source": [
    "* In the above code, you are doing the Linear Discriminant Analysis, in which you find the LDA Accuracy and LDA Classification Report.\n",
    "* Analogous to the principle of beam search, let us stick with the three best algorithms, LDA, RFC, and GBC, and tune these models further, picking the one that gives the highest accuracy after tuning. Instantiate a new LDA model and check its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYWREBDqfdAh"
   },
   "outputs": [],
   "source": [
    "confusion_matrix_lda = pd.DataFrame(confusion_matrix(y_val, lda_predicted),\n",
    "                                  index = ['Actual Negative','Actual Positive'],\n",
    "                        columns = ['Predicted Negative','Predicted Postive'] )\n",
    "confusion_matrix_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asmSLpjTabQ0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIuuR7q8YE5f"
   },
   "source": [
    "**Note: In this lesson, we saw the use of the feature selection methods, but in the next lesson we are going to use one of these methods as a sub-component of \"Supervised Learning - Regression and Classification\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sd-UtEKgZl5Q"
   },
   "source": [
    "![Simplilearn_Logo](https://labcontent.simplicdn.net/data-content/content-assets/Data_and_AI/Logo_Powered_By_Simplilearn/SL_Logo_1.png)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "7.01_Feature_Selection.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
